"""
TranslateGemma ç¿»è­¯å™¨æ¨¡çµ„

è² è²¬è¼‰å…¥ GGUF æ¨¡å‹ä¸¦æä¾›ç¿»è­¯åŠŸèƒ½
é¦–æ¬¡åŸ·è¡Œæ™‚æœƒè‡ªå‹•ä¸‹è¼‰æ¨¡å‹
"""
import os
from pathlib import Path
import platform
from typing import Optional

from huggingface_hub import hf_hub_download
from llama_cpp import Llama


# æ¨¡å‹è¨­å®š
MODEL_REPO = "mradermacher/translategemma-4b-it-GGUF"
MODEL_FILENAME = "translategemma-4b-it.Q6_K.gguf"
MODELS_DIR = Path(__file__).parent / "models"

# èªè¨€ä»£ç¢¼å°æ‡‰ï¼ˆISO 639-1ï¼‰
LANGUAGE_CODES = {
    "en": "English",
    "zh": "Chinese",
    "zh-TW": "Traditional Chinese",
    "zh-CN": "Simplified Chinese", 
    "ja": "Japanese",
    "ko": "Korean",
    "fr": "French",
    "de": "German",
    "es": "Spanish",
    "it": "Italian",
    "pt": "Portuguese",
    "ru": "Russian",
    "ar": "Arabic",
    "th": "Thai",
    "vi": "Vietnamese",
}


class TranslateGemmaTranslator:
    """TranslateGemma ç¿»è­¯å™¨"""
    
    def __init__(self, n_ctx: int = 2048, n_gpu_layers: int = -1):
        """
        åˆå§‹åŒ–ç¿»è­¯å™¨
        
        åƒæ•¸:
            n_ctx: ä¸Šä¸‹æ–‡é•·åº¦
            n_gpu_layers: GPU å±¤æ•¸ï¼Œ-1 è¡¨ç¤ºå…¨éƒ¨ä½¿ç”¨ GPU
        """
        self.model: Optional[Llama] = None
        self.is_loaded = False
        self.n_ctx = n_ctx
        self.n_gpu_layers = n_gpu_layers
        
        # ç¢ºä¿æ¨¡å‹ç›®éŒ„å­˜åœ¨
        MODELS_DIR.mkdir(parents=True, exist_ok=True)
        
        # è¼‰å…¥æ¨¡å‹
        self._load_model()
    
    def _download_model(self) -> Path:
        """ä¸‹è¼‰æ¨¡å‹ï¼ˆè‹¥ä¸å­˜åœ¨ï¼‰"""
        model_path = MODELS_DIR / MODEL_FILENAME
        
        if model_path.exists():
            print(f"âœ… æ¨¡å‹å·²å­˜åœ¨: {model_path}")
            return model_path
        
        print(f"ğŸ“¥ æ­£åœ¨ä¸‹è¼‰æ¨¡å‹ {MODEL_FILENAME}...")
        print(f"   ä¾†æº: {MODEL_REPO}")
        print(f"   é€™å¯èƒ½éœ€è¦å¹¾åˆ†é˜ï¼Œè«‹ç¨å€™...")
        
        downloaded_path = hf_hub_download(
            repo_id=MODEL_REPO,
            filename=MODEL_FILENAME,
            local_dir=MODELS_DIR,
            local_dir_use_symlinks=False
        )
        
        print(f"âœ… æ¨¡å‹ä¸‹è¼‰å®Œæˆ: {downloaded_path}")
        return Path(downloaded_path)
    
    def _load_model(self):
        """è¼‰å…¥ GGUF æ¨¡å‹"""
        model_path = self._download_model()
        
        print(f"ğŸ”„ æ­£åœ¨è¼‰å…¥æ¨¡å‹åˆ°è¨˜æ†¶é«”...")
        backend = detect_backend()
        if backend == "CPU":
            print("âš™ï¸ æ¨è«–æ¨¡å¼: CPU")
        else:
            if self.n_gpu_layers == 0:
                print(f"âš™ï¸ æ¨è«–æ¨¡å¼: CPU (GPU å¾Œç«¯ {backend} å¯ç”¨ï¼Œä½† n_gpu_layers=0)")
            else:
                print(f"âš™ï¸ æ¨è«–æ¨¡å¼: {backend} GPU (n_gpu_layers={self.n_gpu_layers})")
        
        self.model = Llama(
            model_path=str(model_path),
            n_ctx=self.n_ctx,
            n_gpu_layers=self.n_gpu_layers,
            verbose=False
        )
        
        self.is_loaded = True
        print("âœ… æ¨¡å‹è¼‰å…¥å®Œæˆï¼")

    def translate(
        self,
        text: str,
        source_lang: str = "en",
        target_lang: str = "zh-TW"
    ) -> str:
        """
        ç¿»è­¯æ–‡å­—

        åƒæ•¸:
            text: å¾…ç¿»è­¯çš„æ–‡å­—
            source_lang: ä¾†æºèªè¨€ä»£ç¢¼
            target_lang: ç›®æ¨™èªè¨€ä»£ç¢¼

        å›å‚³:
            ç¿»è­¯å¾Œçš„æ–‡å­—
        """
        if not self.is_loaded or not self.model:
            raise RuntimeError("æ¨¡å‹å°šæœªè¼‰å…¥")

        # å–å¾—èªè¨€åç¨±
        source_name = LANGUAGE_CODES.get(source_lang, source_lang)
        target_name = LANGUAGE_CODES.get(target_lang, target_lang)

        # å»ºæ§‹ TranslateGemma å°ˆç”¨çš„ prompt æ ¼å¼
        # åƒè€ƒ: https://huggingface.co/google/translategemma-4b-it
        prompt = f"""<start_of_turn>user
Translate the following text from {source_name} to {target_name}:

{text}<end_of_turn>
<start_of_turn>model
"""

        # é€²è¡Œæ¨è«–
        response = self.model(
            prompt,
            max_tokens=self.n_ctx,
            stop=["<end_of_turn>", "<eos>"],
            echo=False
        )

        # æå–ç¿»è­¯çµæœ
        translation = response["choices"][0]["text"].strip()

        return translation


def detect_backend() -> str:
    """Best-effort æª¢æ¸¬å¯ç”¨çš„ GPU å¾Œç«¯"""
    try:
        import llama_cpp  # å±€éƒ¨åŒ¯å…¥é¿å…å•Ÿå‹•æˆæœ¬
        lib_dir = Path(llama_cpp.__file__).parent / "lib"
    except Exception:
        return "CPU"

    if platform.system() == "Darwin":
        if (lib_dir / "libggml-metal.dylib").exists():
            return "Metal"

    for pattern in ("*cuda*.dll", "*cuda*.so", "*cuda*.dylib"):
        if any(lib_dir.glob(pattern)):
            return "CUDA"

    return "CPU"


if __name__ == "__main__":
    # æ¸¬è©¦ç¿»è­¯å™¨
    print("ğŸ§ª æ¸¬è©¦ TranslateGemma ç¿»è­¯å™¨")
    
    translator = TranslateGemmaTranslator()
    
    test_texts = [
        ("Hello, how are you?", "en", "zh-TW"),
        ("ä»Šå¤©å¤©æ°£å¾ˆå¥½", "zh-TW", "en"),
    ]
    
    for text, src, tgt in test_texts:
        print(f"\nğŸ“ åŸæ–‡ ({src}): {text}")
        result = translator.translate(text, src, tgt)
        print(f"ğŸ“– è­¯æ–‡ ({tgt}): {result}")
